{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://duckdb.org/images/DuckDB-Footer.svg\" alt=\"DuckDB\" width=\"200\"/>\n",
    "\n",
    "# <h1 align='center'>[DuckDB](https://duckdb.org/)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instalação\n",
    "!pip install duckdb==0.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libs\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processando os dados com `CSV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lendo um arquivo `CSV`\n",
    "    A função `duckdb.sql` será usada para executar a consulta SQL especificada dentro das aspas duplas. \n",
    "    A consulta seleciona todos os dados do arquivo __CSV__ (\"campeonato-brasileiro-cartoes.csv\") usando a função `read_csv_auto` e, em seguida, o método `show()` é chamado para exibir o resultado da consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT * FROM read_csv_auto('./Campeonato Brasileiro de futebol/campeonato-brasileiro-cartoes.csv');\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __classe__ `duckdb.connect()` cria um contexto de banco de dados, o __método__ conexao.execute() realiza a criação da tabela e depois realiza uma consulta _SQL_ nesta tabela salvando o resultado como um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a conexão com DuckDB\n",
    "conexao = duckdb.connect()\n",
    "\n",
    "# Carrega os dados do CSV para a tabela\n",
    "conexao.execute(\"CREATE TABLE campeonato_brasileiro_cartoes AS (SELECT * FROM './Campeonato Brasileiro de futebol/campeonato-brasileiro-cartoes.csv')\")\n",
    "\n",
    "# Executa consulta SQL na tabela\n",
    "resultado = conexao.execute(\"SELECT * FROM campeonato_brasileiro_cartoes\")\n",
    "\n",
    "# Transforma o resultado em um DataFrame\n",
    "resultado.fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa consulta SQL na tabela\n",
    "resultado = conexao.execute(\"SELECT * FROM campeonato_brasileiro_cartoes\")\n",
    "\n",
    "# Transforma o resultado em um DataFrame\n",
    "dados = resultado.fetch_df()\n",
    "\n",
    "# Salva o dataframe como um novo csv\n",
    "dados.to_csv(\"exemplo_futebol.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `json`, `parquet`, `xlsx`, `xml`\n",
    "Os mesmos processos de \"conexão\" ou \"setup\" servem para os demais tipos de arquivos suportados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Função Glob para encontrar nomes de arquivos\n",
    "duckdb.sql(\"SELECT * FROM glob('./exemplo_arquivos/*')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "duckdb.sql(\"SELECT * FROM ./exemplo_arquivos/exemplo_futebol.csv;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "duckdb.sql(\"SELECT * FROM ./exemplo_arquivos/exemplo_futebol.json;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet\n",
    "duckdb.sql(\"SELECT * FROM ./exemplo_arquivos/exemplo_futebol.parquet;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark, Polars e Pandas\n",
    "DuckDB funciona muito bem também com `Spark`, `Polars` e `Pandas` é possível usar essas stacks para realizar alguma tarefa, como ler e pré-processar um dado, e após isso usar o DuckDB para conclusão, ou o inverso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Usando padnas\n",
    "import pandas as pd\n",
    "\n",
    "dado_lido = pd.read_excel('.exemplo_arquivos/exemplo_futebol.xlsx')\n",
    "\n",
    "# pandas\n",
    "duckdb.sql(\"SELECT * FROM dado_lido;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O inverso também é possível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "dado_recebido = duckdb.sql(\"SELECT * FROM dado_lido;\").to_df()\n",
    "\n",
    "print(f\"Tipo do dataframe: `dado_recebido` é {type(dado_recebido)}\")\n",
    "dado_recebido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruzamento de dados\n",
    "Quando falamos em pipeline de dados, transformação, muitas vezes estamos querendo enriquecer uma base com outra base, e para isso nada melhor que SQL para relacionar, mas imagina você quer cruzar os dados de um `json` com um `parquet`.\n",
    "Isso é complicado fazer até mesmo com `Pyspark`, `Pandas`, `Polars`, mas com `DuckDB`, é muito simples, vamos ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando como `csv`\n",
    "duckdb.sql(\"SELECT * FROM './Campeonato Brasileiro de futebol/campeonato-brasileiro-cartoes.csv'\").to_csv(\"./uniao_tabelas/campeonato-brasileiro-cartoes.csv\")\n",
    "# Salvando como `parquet`\n",
    "duckdb.sql(\"SELECT * FROM './Campeonato Brasileiro de futebol/campeonato-brasileiro-gols.csv'\").to_parquet(\"./uniao_tabelas/campeonato-brasileiro-gols.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__IMPORTANTE__\n",
    ">\n",
    "> _Observação_: Não estamos considerando boas práticas de SQL ou tratamento de dados aqui no momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo `csv` e `parquet`\n",
    "duckdb.sql(\"SELECT partida_cartoes.atleta, COUNT(DISTINCT(partida_cartoes.partida_id, partida_cartoes.atleta, partida_cartoes.minuto)) AS total_cartoes_recebidos, COUNT(DISTINCT(partida_gols.partida_id, partida_gols.atleta, partida_gols.minuto)) AS total_gols_Feitos FROM './uniao_tabelas/campeonato-brasileiro-cartoes.csv' AS partida_cartoes JOIN './uniao_tabelas/campeonato-brasileiro-gols.parquet' AS partida_gols ON partida_cartoes.partida_id = partida_gols.partida_id AND partida_cartoes.atleta = partida_gols.atleta AND partida_cartoes.clube = partida_gols.clube GROUP BY partida_cartoes.atleta ORDER BY COUNT(partida_gols.atleta) DESC, COUNT(partida_cartoes.atleta) DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continua...\n",
    "Você conseguiu ve até aqui quão poderosa essa ferramenta é? com uma única linha, conseguimos unir duas tabelas geradas de dois tipos de arquivos diferentes, e não para por ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribuição\n",
    "\n",
    "Se você tiver alguma sugestão ou quiser contribuir com materiais adicionais, sinta-se à vontade para abrir uma issue ou enviar um pull request neste repositório.\n",
    "\n",
    "Divirta-se aprendendo!\n",
    "\n",
    "### Autor\n",
    "\n",
    "(Gustavo Lopes: [GitHub](https://github.com/Gustavo-H-Martins) | [LinkedIn](https://www.linkedin.com/in/gustavo-henrique-lopes-martins-361789192/))\n",
    "\n",
    "![Gustavo Lopes](https://media.licdn.com/dms/image/D4D03AQHV5drm3wpahA/profile-displayphoto-shrink_100_100/0/1690910388427?e=1705536000&v=beta&t=aJWHFAWbByEHIyIBM1o6m3zfBB8arlyMEQIpP7ruRJk)\n",
    "\n",
    "- Engenheiro de Dados com foco\n",
    "  -\n",
    "  - Big Data\n",
    "  - Processamento Distribuído\n",
    "  - Arquitetura e computação em nuvem\n",
    "  - Performance e redução de custo\n",
    "  - Repasse de conhecimento e democratização de acesso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_knowlege-hW8H2qLw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

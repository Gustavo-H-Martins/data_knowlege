{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um Acionador do Spark que recebe um sparksession, sparkcontext e retorna um build com as dependências jars instanciadas, tamanho da memória e quantidade de núcleos usados e máximo, e também o nível de log só como \"ERROR\"\n",
    "def spark_builder(sparksession, sparkcontext):\n",
    "  # Instanciar as dependências jars\n",
    "  jars = [\"spark-sql-kafka-0-10_2.12-3.1.2.jar\", \"spark-avro_2.12-3.1.2.jar\", \"delta-core_2.12-1.0.0.jar\"] # Exemplo de jars, pode variar de acordo com o projeto\n",
    "  sparksession.conf.set(\"spark.jars\", \",\".join(jars))\n",
    "\n",
    "  # Configurar o tamanho da memória e a quantidade de núcleos usados e máximo\n",
    "  sparksession.conf.set(\"spark.executor.memory\", \"4g\") # Exemplo de memória, pode variar de acordo com o projeto\n",
    "  sparksession.conf.set(\"spark.executor.cores\", \"2\") # Exemplo de núcleos, pode variar de acordo com o projeto\n",
    "  sparksession.conf.set(\"spark.cores.max\", \"4\") # Exemplo de núcleos máximos, pode variar de acordo com o projeto\n",
    "\n",
    "  # Configurar o nível de log só como \"ERROR\"\n",
    "  sparkcontext.setLogLevel(\"ERROR\")\n",
    "\n",
    "  # Retornar o build\n",
    "  return sparksession.builder.getOrCreate()\n",
    "\n",
    "# Uma função de download das dependências maven que recebe o link do repositório central maven e faz o download (usando só python)\n",
    "def maven_downloader(repo_url):\n",
    "  # Importar a biblioteca pysmartdl para fazer o download\n",
    "  from pysmartdl import SmartDL\n",
    "\n",
    "  # Criar uma lista de dependências maven (exemplo, pode variar de acordo com o projeto)\n",
    "  dependencies = [\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\", \"org.apache.spark:spark-avro_2.12:3.1.2\", \"io.delta:delta-core_2.12:1.0.0\"]\n",
    "\n",
    "  # Iterar sobre as dependências e fazer o download de cada uma\n",
    "  for dependency in dependencies:\n",
    "    # Separar o grupo, o artefato e a versão da dependência\n",
    "    group, artifact, version = dependency.split(\":\")\n",
    "    # Construir o link do jar da dependência\n",
    "    jar_url = repo_url + \"/\" + group.replace(\".\", \"/\") + \"/\" + artifact + \"/\" + version + \"/\" + artifact + \"-\" + version + \".jar\"\n",
    "    # Criar um objeto SmartDL com o link do jar\n",
    "    downloader = SmartDL(jar_url)\n",
    "    # Iniciar o download do jar\n",
    "    downloader.start()\n",
    "    # Imprimir uma mensagem de sucesso\n",
    "    print(f\"Downloaded {artifact}-{version}.jar from {jar_url}\")\n",
    "\n",
    "# Uma função que recebe um diretório de origem para mapear todos os arquivos avro de um repositório no s3 e criar um dataframe, retorna um dataframe delta\n",
    "def avro_to_delta(source_dir):\n",
    "  # Importar as bibliotecas spark, delta e boto3\n",
    "  from pyspark.sql import SparkSession\n",
    "  from delta import DeltaTable\n",
    "  import boto3\n",
    "\n",
    "  # Criar um objeto sparksession usando a função spark_builder\n",
    "  spark = spark_builder(SparkSession.builder, SparkSession.builder.getOrCreate().sparkContext)\n",
    "\n",
    "  # Criar um objeto boto3 para acessar o s3\n",
    "  s3 = boto3.resource(\"s3\")\n",
    "\n",
    "  # Criar uma lista vazia para armazenar os caminhos dos arquivos avro\n",
    "  avro_files = []\n",
    "\n",
    "  # Iterar sobre os objetos do diretório de origem no s3\n",
    "  for obj in s3.Bucket(\"my-bucket\").objects.filter(Prefix=source_dir): # Exemplo de bucket, pode variar de acordo com o projeto\n",
    "    # Verificar se o objeto é um arquivo avro\n",
    "    if obj.key.endswith(\".avro\"):\n",
    "      # Adicionar o caminho do arquivo avro à lista\n",
    "      avro_files.append(f\"s3a://{obj.bucket_name}/{obj.key}\")\n",
    "\n",
    "  # Criar um dataframe spark a partir dos arquivos avro usando o formato avro\n",
    "  df = spark.read.format(\"avro\").load(avro_files)\n",
    "\n",
    "  # Criar um dataframe delta a partir do dataframe spark usando o formato delta\n",
    "  delta_df = df.write.format(\"delta\").saveAsTable(\"my_table\") # Exemplo de tabela, pode variar de acordo com o projeto\n",
    "\n",
    "  # Retornar o dataframe delta\n",
    "  return delta_df\n",
    "\n",
    "# Uma função que recebe um dataframe delta e faz os processamentos de uma camada bronze\n",
    "def bronze_processing(delta_df):\n",
    "  # Importar as bibliotecas spark e delta\n",
    "  from pyspark.sql import SparkSession\n",
    "  from delta import DeltaTable\n",
    "\n",
    "  # Criar um objeto sparksession usando a função spark_builder\n",
    "  spark = spark_builder(SparkSession.builder, SparkSession.builder.getOrCreate().sparkContext)\n",
    "\n",
    "  # Criar um objeto deltatable a partir do dataframe delta\n",
    "  delta_table = DeltaTable.forPath(spark, delta_df)\n",
    "\n",
    "  # Fazer os processamentos de uma camada bronze (exemplo, pode variar de acordo com o projeto)\n",
    "  # Adicionar uma coluna com a data e hora da ingestão dos dados\n",
    "  delta_table.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "  # Adicionar uma coluna com o hash dos dados para identificar duplicatas\n",
    "  delta_table.withColumn(\"hash\", md5(concat_ws(\"|\", *delta_table.columns)))\n",
    "  # Remover as duplicatas usando o hash\n",
    "  delta_table.dropDuplicates([\"hash\"])\n",
    "  # Retornar o objeto deltatable\n",
    "  return delta_table\n",
    "\n",
    "# Uma função que recebe um dataframe delta da camada anterior e faz os processamentos da camada silver\n",
    "def silver_processing(bronze_delta_df):\n",
    "  # Importar as bibliotecas spark e delta\n",
    "  from pyspark.sql import SparkSession\n",
    "  from delta import DeltaTable\n",
    "\n",
    "  # Criar um objeto sparksession usando a função spark_builder\n",
    "  spark = spark_builder(SparkSession.builder, SparkSession.builder.getOrCreate().sparkContext)\n",
    "\n",
    "  # Criar um objeto deltatable a partir do dataframe delta da camada bronze\n",
    "  bronze_delta_table = DeltaTable.forPath(spark, bronze_delta_df)\n",
    "\n",
    "  # Fazer os processamentos da camada silver (exemplo, pode variar de acordo com o projeto)\n",
    "  # Criar uma visão temporária a partir do deltatable da camada bronze\n",
    "  bronze_delta_table.createOrReplaceTempView(\"bronze_view\")\n",
    "  # Criar um dataframe spark a partir de uma consulta SQL que aplica as transformações desejadas\n",
    "  silver_df = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    movie_id, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "    movie_title, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "    movie_genre, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "    user_id, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "    user_rating, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "    ingestion_timestamp, -- Coluna da camada bronze\n",
    "    hash -- Coluna da camada bronze\n",
    "  FROM bronze_view\n",
    "  WHERE user_rating IS NOT NULL -- Exemplo de filtro, pode variar de acordo com o projeto\n",
    "  \"\"\")\n",
    "  # Criar um dataframe delta a partir do dataframe spark usando o formato delta\n",
    "  silver_delta_df = silver_df.write.format(\"delta\").saveAsTable(\"silver_table\") # Exemplo de tabela, pode variar de acordo com o projeto\n",
    "\n",
    "  # Retornar o dataframe delta da camada silver\n",
    "  return silver_delta_df\n",
    "\n",
    "# Uma função que recebe um ou mais diretórios da camada silver delta lake e faz os processamentos da camada gold\n",
    "def gold_processing(silver_delta_dirs):\n",
    "  # Importar as bibliotecas spark e delta\n",
    "  from pyspark.sql import SparkSession\n",
    "  from delta import DeltaTable\n",
    "\n",
    "  # Criar um objeto sparksession usando a função spark_builder\n",
    "  spark = spark_builder(SparkSession.builder, SparkSession.builder.getOrCreate().sparkContext)\n",
    "\n",
    "  # Criar uma lista vazia para armazenar os objetos deltatable da camada silver\n",
    "  silver_delta_tables = []\n",
    "\n",
    "  # Iterar sobre os diretórios da camada silver\n",
    "  for silver_delta_dir in silver_delta_dirs:\n",
    "    # Criar um objeto deltatable a partir do diretório\n",
    "    silver_delta_table = DeltaTable.forPath(spark, silver_delta_dir)\n",
    "    # Adicionar o objeto deltatable à lista\n",
    "    silver_delta_tables.append(silver_delta_table)\n",
    "\n",
    "  # Fazer os processamentos da camada gold (exemplo, pode variar de acordo com o projeto)\n",
    "  # Criar um dataframe spark vazio para armazenar os dados da camada gold\n",
    "  gold_df = spark.createDataFrame([], schema=None)\n",
    "  # Iterar sobre os objetos deltatable da camada silver\n",
    "  for silver_delta_table in silver_delta_tables:\n",
    "    # Criar uma visão temporária a partir do deltatable da camada silver\n",
    "    silver_delta_table.createOrReplaceTempView(\"silver_view\")\n",
    "    # Criar um dataframe spark a partir de uma consulta SQL que aplica as agregações desejadas\n",
    "    gold_df_temp = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      movie_id, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "      movie_title, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "      movie_genre, -- Exemplo de coluna, pode variar de acordo com o projeto\n",
    "      AVG(user_rating) AS average_rating, -- Exemplo de agregação, pode variar de acordo com o projeto\n",
    "      COUNT(user_id) AS number_of_users -- Exemplo de agregação, pode variar de acordo com o projeto\n",
    "    FROM silver_view\n",
    "    GROUP BY movie_id, movie_title, movie_genre -- Exemplo de agrupamento, pode variar de acordo com o projeto\n",
    "    \"\"\")\n",
    "    # Unir o dataframe spark temporário com o dataframe spark da camada gold\n",
    "    gold_df = gold_df.union(gold_df_temp)\n",
    "\n",
    "  # Criar um dataframe delta a partir do dataframe spark da camada gold usando o formato delta\n",
    "  gold_delta_df = gold_df.write.format(\"delta\").saveAsTable(\"gold_table\") # Exemplo de tabela, pode variar de acordo com o projeto\n",
    "\n",
    "  # Retornar o dataframe delta da camada gold\n",
    "  return gold_delta_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
